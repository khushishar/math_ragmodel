{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Installations"
      ],
      "metadata": {
        "id": "StUSACM2RYGA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GU03ZWNg8gz1"
      },
      "outputs": [],
      "source": [
        "!pip install -U langchain-community\n",
        "!pip install agno\n",
        "!pip install google-search-results\n",
        "!pip install chromadb\n",
        "!pip install groq"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting knowlegebase from kaggle\n"
      ],
      "metadata": {
        "id": "vat1b9WsResR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CoHLkwA9UcT"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "path = kagglehub.dataset_download(\"thedevastator/dataset-for-solving-math-word-problems\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wj1pa4sw9aKv",
        "outputId": "0c1bc614-2f06-4d49-b231-8e0b066817f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/dataset-for-solving-math-word-problems\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Problem', 'Rationale', 'options', 'correct', 'annotated_formula',\n",
              "       'linear_formula', 'category'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import os\n",
        "print(\"Path to dataset files:\", path)\n",
        "os.path.exists(path)\n",
        "data= os.path.join(path,\"train.csv\")\n",
        "import pandas as pd\n",
        "from langchain.docstore.document import Document\n",
        "df = pd.read_csv(data)\n",
        "\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Processing the data for vector databse"
      ],
      "metadata": {
        "id": "aF8CKTl1RmxI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WxqQAN79pOz"
      },
      "outputs": [],
      "source": [
        "docs = []\n",
        "for _, entry in df.iterrows():\n",
        "    content = f\"\"\"Question: {entry['Problem']} {entry['options']}\n",
        "Answer: {entry['Rationale']} | Correct: {entry['correct']}\n",
        "Formulae: {entry['annotated_formula']},{entry['linear_formula']}\n",
        "Category: {entry['category']}\"\"\"\n",
        "\n",
        "    doc = Document(page_content=content, metadata={\"answer\": entry[\"correct\"]})\n",
        "    docs.append(doc)\n",
        "\n",
        "# Optional: limit for testing\n",
        "docs = docs[:2000]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "embedding the data and making the vectordb\n"
      ],
      "metadata": {
        "id": "NYNOHZjyR5Lk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BU162Xpb66V7"
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "from tqdm import tqdm\n",
        "from uuid import uuid4\n",
        "model_name = \"BAAI/bge-base-en-v1.5\"\n",
        "model_kwargs = {\"device\": \"cpu\"}\n",
        "encode_kwargs = {\"normalize_embeddings\": True}\n",
        "\n",
        "hf = HuggingFaceBgeEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs=model_kwargs,\n",
        "    encode_kwargs=encode_kwargs\n",
        ")\n",
        "\n",
        "vector_store = Chroma(\n",
        "    collection_name=\"example_collection\",\n",
        "    embedding_function=hf,\n",
        "    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n",
        ")\n",
        "\n",
        "\n",
        "uuids = [str(uuid4()) for _ in range(len(docs))]\n",
        "\n",
        "vector_store.add_documents(documents=docs, ids=uuids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jb5ECNJgCWeU",
        "outputId": "d7f6f764-23f3-4799-a765-785bc86fcbac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2000\n"
          ]
        }
      ],
      "source": [
        "print(vector_store._collection.count())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up agents for efficient swith between database and web search also integrating a Streamlit UI\n",
        "\n"
      ],
      "metadata": {
        "id": "3b-O6S3pSD83"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PLDe85yP_C0",
        "outputId": "19967d94-9065-4f02-fd00-9f5a62bed558"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import json\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "\n",
        "from agno.models.groq import Groq\n",
        "from agno.agent import Agent\n",
        "from agno.tools.serpapi import SerpApiTools\n",
        "from agno.embedder.huggingface import HuggingfaceCustomEmbedder\n",
        "from agno.knowledge.pdf_url import PDFUrlKnowledgeBase\n",
        "from agno.vectordb.chroma import ChromaDb\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=ResourceWarning)\n",
        "\n",
        "MODEL_ID = \"llama-3.3-70b-versatile\"\n",
        "FEEDBACK_LOG = \"feedback_log.jsonl\"\n",
        "\n",
        "my_model = Groq(id=MODEL_ID)\n",
        "\n",
        "classifier_agent = Agent(\n",
        "    name=\"Classifier Agent\",\n",
        "    role=\"Classify questions\",\n",
        "    model=my_model,\n",
        "    instructions=\"Return only 'Math' or 'Not Math' based on the question.\",\n",
        "    show_tool_calls=False,\n",
        "    markdown=False,\n",
        ")\n",
        "\n",
        "def classify_question_type(question: str) -> str:\n",
        "    prompt = f\"Classify the following question as either 'Math' or 'Not Math':\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
        "    result = classifier_agent.run(prompt)\n",
        "    return result.content.strip()\n",
        "\n",
        "\n",
        "web_agent = Agent(\n",
        "    name=\"Web Agent\",\n",
        "    role=\"Search the web for information\",\n",
        "    model=my_model,\n",
        "    tools=[SerpApiTools()],\n",
        "    instructions=\"Always include sources\",\n",
        "    show_tool_calls=True,\n",
        "    markdown=True,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "knowledge_base = vector_store\n",
        "\n",
        "\n",
        "\n",
        "knowledge_base.load(recreate=False)  #used for the loading the database will comment after first run\n",
        "\n",
        "agent_team = Agent(\n",
        "    team=[web_agent],\n",
        "    model=my_model,\n",
        "    knowledge=knowledge_base,\n",
        "    instructions=[\"Always include sources\", \"Use tables to display data\"],\n",
        "    show_tool_calls=True,\n",
        "    markdown=True,\n",
        ")\n",
        "\n",
        "\n",
        "st.set_page_config(page_title=\"Math Agent\", layout=\"wide\")\n",
        "st.title(\"üßÆ Your Math Professor\")\n",
        "\n",
        "\n",
        "if \"response\" not in st.session_state:\n",
        "    st.session_state.response = None\n",
        "if \"question\" not in st.session_state:\n",
        "    st.session_state.question = \"\"\n",
        "if \"feedback\" not in st.session_state:\n",
        "    st.session_state.feedback = None\n",
        "\n",
        "\n",
        "with st.form(\"question_form\"):\n",
        "    question = st.text_input(\"Enter a math question:\", placeholder=\"e.g., What is the derivative of x^3 + 3\")\n",
        "    submit = st.form_submit_button(\"Ask Agent\")\n",
        "\n",
        "\n",
        "if submit and question:\n",
        "    q_type = classify_question_type(question)\n",
        "    if q_type.lower() != \"math\":\n",
        "        st.error(\"‚ùå Only math-related questions are allowed. Try rephrasing.\")\n",
        "    else:\n",
        "        with st.spinner(\"Thinking...\"):\n",
        "            response = agent_team.run(question)\n",
        "            st.session_state.response = response.content\n",
        "            st.session_state.question = question\n",
        "            st.session_state.feedback = None\n",
        "\n",
        "\n",
        "if st.session_state.response:\n",
        "    st.markdown(\"### ü§ñ Agent's Answer\")\n",
        "    st.markdown(st.session_state.response)\n",
        "\n",
        "    st.markdown(\"### üß† Human Feedback\")\n",
        "    st.session_state.feedback = st.radio(\n",
        "        \"Was the answer helpful?\",\n",
        "        [\"Yes\", \"No\"],\n",
        "        key=\"feedback_radio\"\n",
        "    )\n",
        "\n",
        "    corrected_answer = \"\"\n",
        "    if st.session_state.feedback == \"No\":\n",
        "        corrected_answer = st.text_area(\n",
        "            \"Provide a corrected answer (optional):\",\n",
        "            key=\"corrected_answer\"\n",
        "        )\n",
        "\n",
        "    if st.button(\"Submit Feedback\"):\n",
        "        log = {\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"question\": st.session_state.question,\n",
        "            \"agent_answer\": st.session_state.response,\n",
        "            \"feedback\": st.session_state.feedback,\n",
        "            \"corrected_answer\": corrected_answer if st.session_state.feedback == \"No\" else None\n",
        "        }\n",
        "\n",
        "        with open(FEEDBACK_LOG, \"a\") as f:\n",
        "            f.write(json.dumps(log) + \"\\n\")\n",
        "\n",
        "        st.success(\"‚úÖ Feedback logged\")\n",
        "\n",
        "        if corrected_answer:\n",
        "            vector_store.add_documents([Document(page_content=f\"Question: {st.session_state.question}, Answer: {corrected_answer}\",metadata={\"Answer\":corrected_answer})],ids=[str(uuid4())])\n",
        "            st.info(\"üìö Corrected answer added to knowledge base\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aljRaPnc3ARg",
        "outputId": "7e17762c-df14-4bc1-8f17-3781bb70e0f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.16.36.207"
          ]
        }
      ],
      "source": [
        "!curl https://loca.lt/mytunnelpassword"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSX1gUXl3Wco"
      },
      "outputs": [],
      "source": [
        "!streamlit run app.py &>/content/logs.txt &"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FAM3dlt4kBU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3114c072-5f84-4d1a-be16-4a56c19489ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K\u001b[1G\u001b[0JNeed to install the following packages:\n",
            "localtunnel@2.0.2\n",
            "Ok to proceed? (y) \u001b[20Gy\n",
            "\n",
            "\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0Kyour url is: https://chubby-clocks-tie.loca.lt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!npx localtunnel --port 8501"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}